{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_extra.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y83jLZl8z9FY",
        "IQnfhcGo1TEl",
        "xh4lXI9A18ia",
        "-b1L8lOb0Zwv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQAWdtx6dHAV"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfaygZiz25MS"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbDK2HScgEvr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel \n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier , VotingClassifier\n",
        "from sklearn.metrics import accuracy_score , f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y83jLZl8z9FY"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyPke2fgKNu"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/nlp/HW3/train1.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/nlp/HW3/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtTAWxCjgStr"
      },
      "source": [
        "def remove_tag(contex):\n",
        "  contex = contex.replace(\"<head>\" , \"\")\n",
        "  contex = contex.replace(\"</head>\" , \"\")\n",
        "  return contex\n",
        "\n",
        "import re\n",
        "def find_head(contex):\n",
        "  match = re.findall(r'<head>\\w+</head>' , contex)\n",
        "  return match[0][6:-7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfA3uhOwg6o9"
      },
      "source": [
        "train_data['head'] = train_data['context'].apply(find_head)\n",
        "train_data['context'] = train_data['context'].apply(remove_tag)\n",
        "\n",
        "test_data['head'] = test_data['context'].apply(find_head)\n",
        "test_data['context'] = test_data['context'].apply(remove_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L9vYw3I1gcb"
      },
      "source": [
        "train_data = train_data.drop(['instance_id' , 'doc_src' ]  , axis=1)\n",
        "train_data = train_data.rename(columns={'sense_id': 'label'})\n",
        "test_data = test_data.drop(['instance_id' , 'doc_src']  , axis=1)\n",
        "test_data = test_data.rename(columns={'sense_id': 'label'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVuD2kPd1zz2"
      },
      "source": [
        "train_data['sent'] = train_data['context'].apply(lambda x : x.split('.'))\n",
        "test_data['sent'] = test_data['context'].apply(lambda x : x.split('.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Kh7QY22RpK"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for i in range(len(train_data)):\n",
        "  main_sent = ''\n",
        "  for s in train_data['sent'][i]:\n",
        "    tokens = tokenizer.tokenize(s)\n",
        "    if train_data['head'][i] in tokens: main_sent = s\n",
        "  train_data['sent'][i] = main_sent "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nra0sW1P5uYc"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for i in range(len(test_data)):\n",
        "  main_sent = ''\n",
        "  for s in test_data['sent'][i]:\n",
        "    tokens = tokenizer.tokenize(s)\n",
        "    if test_data['head'][i] in tokens: main_sent = s\n",
        "  test_data['sent'][i] = main_sent "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ad0tJlP00-A"
      },
      "source": [
        "using transformers tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G6p1R7G3E8K"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNxe8wo06-w"
      },
      "source": [
        "add [CLS] and [SEP] tokens to the main sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFY7ILfr3NEs"
      },
      "source": [
        "train_data['sent'] = train_data['sent'].apply(lambda x : \"[CLS] \" + x + \" [SEP]\" )\n",
        "train_data['tokens'] = train_data['sent'].apply(tokenizer.tokenize)\n",
        "train_data['indx'] = train_data['tokens'].apply(tokenizer.convert_tokens_to_ids)\n",
        "train_data['segments_ids'] = train_data['tokens'].apply(lambda x : [1] * len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZrNj3Yhgp2O"
      },
      "source": [
        "test_data['sent'] = test_data['sent'].apply(lambda x : \"[CLS] \" + x + \" [SEP]\" )\n",
        "test_data['tokens'] = test_data['sent'].apply(tokenizer.tokenize)\n",
        "test_data['indx'] = test_data['tokens'].apply(tokenizer.convert_tokens_to_ids)\n",
        "test_data['segments_ids'] = test_data['tokens'].apply(lambda x : [1] * len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnfhcGo1TEl"
      },
      "source": [
        "# Bert Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V87-H3HB1VOd"
      },
      "source": [
        "calculate vectors that the bert model needs in its input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_-8_Bv1_tDI"
      },
      "source": [
        "train_data['tokens_tensor'] = train_data['indx'].apply(lambda x : torch.tensor([x]))\n",
        "train_data['segments_tensors'] = train_data['segments_ids'].apply(lambda x : torch.tensor([x]))\n",
        "\n",
        "test_data['tokens_tensor'] = test_data['indx'].apply(lambda x : torch.tensor([x]))\n",
        "test_data['segments_tensors'] = test_data['segments_ids'].apply(lambda x : torch.tensor([x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0mj5Ivc1gV4"
      },
      "source": [
        "embedd tokens wiht transformer bert model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGcY38Hs-DKb"
      },
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True )\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPG1YUfRjgNi"
      },
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(train_data['tokens_tensor'][10],train_data['segments_tensors'][10])\n",
        "  x = outputs[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR7HP6k9A6_c"
      },
      "source": [
        "train_data['embedding'] = None\n",
        "for i in range(len(train_data)):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(train_data['tokens_tensor'][i],train_data['segments_tensors'][i])\n",
        "  train_data['embedding'][i] = outputs[2]\n",
        "\n",
        "test_data['embedding'] = None\n",
        "for i in range(len(test_data)):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(test_data['tokens_tensor'][i],test_data['segments_tensors'][i])\n",
        "  test_data['embedding'][i] = outputs[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZOsLp8xcI_2"
      },
      "source": [
        "for i in range(len(train_data)):\n",
        "  a = torch.stack(train_data['embedding'][i], dim=0)\n",
        "  a = torch.squeeze(a, dim=1)\n",
        "  a = a.numpy()\n",
        "  train_data['embedding'][i] = a\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "  a = torch.stack(test_data['embedding'][i], dim=0)\n",
        "  a = torch.squeeze(a, dim=1)\n",
        "  a = a.numpy()\n",
        "  test_data['embedding'][i] = a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh4lXI9A18ia"
      },
      "source": [
        "# MLP Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ZnEhxx2ADS"
      },
      "source": [
        "extract [CLS] embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28W2xFLeB6S"
      },
      "source": [
        "train_data['cls'] = None\n",
        "for i in range(len(train_data)):\n",
        "  train_data['cls'][i] = train_data['embedding'][i][12][0]\n",
        "\n",
        "test_data['cls'] = None\n",
        "for i in range(len(test_data)):\n",
        "  test_data['cls'][i] = test_data['embedding'][i][12][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47NP4yXqeqFR"
      },
      "source": [
        "word_list = train_data['word'].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkMDvFOZxser"
      },
      "source": [
        "dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  dataset[word]=train_data.loc[train_data['word']==word_list[id]]\n",
        "\n",
        "test_dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  test_dataset[word]=test_data.loc[test_data['word']==word_list[id]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_-YQc-jxses"
      },
      "source": [
        "label_tansformers = {}\n",
        "for word in word_list:\n",
        "  label_tansformers[word] = preprocessing.LabelEncoder().fit(dataset[word]['label'].values)\n",
        "  dataset[word]['label'] = label_tansformers[word].transform(dataset[word]['label'].values)\n",
        "  test_dataset[word]['label'] = test_dataset[word]['label'].map(lambda s: '<unknown>' if s not in label_tansformers[word].classes_ else s)\n",
        "  label_tansformers[word].classes_ = np.append(label_tansformers[word].classes_, '<unknown>')\n",
        "  test_dataset[word]['label'] = label_tansformers[word].transform(test_dataset[word]['label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmIye4w2GDJ"
      },
      "source": [
        "train MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jOdwwL4IMUS",
        "outputId": "d309ab9a-0655-4922-fcd4-a1a181eeab00"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "acc = []\n",
        "f1 = []\n",
        "for word in word_list:\n",
        "  x = np.vstack(dataset[word]['cls'].values)\n",
        "  y = dataset[word]['label'].values\n",
        "  nn = MLPClassifier(hidden_layer_sizes=(256,) ,activation='relu' , solver='adam' )\n",
        "  nn.fit(x,y)\n",
        "  xx = np.vstack(test_dataset[word]['cls'].values)\n",
        "  yy = test_dataset[word]['label'].values\n",
        "  yp = nn.predict(xx)\n",
        "  acc.append(accuracy_score(yy,yp))\n",
        "  f1.append(f1_score(yy,yp , average='weighted'))\n",
        "\n",
        "print (f'Accuracy => {100 * np.mean(acc):0.2f} %' )\n",
        "print (f'F1-Measure => {np.mean(f1):0.2f}' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 56.67 %\n",
            "F1-Measure => 0.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZlueTt1EQA3"
      },
      "source": [
        "pos_tags = train_data['pos'].unique().tolist()\n",
        "for word in word_list:\n",
        "  verb = train_data[train_data['pos'] == pos_tags[0]]['word'].unique().tolist()\n",
        "  noun = train_data[train_data['pos'] == pos_tags[1]]['word'].unique().tolist()\n",
        "  adj = train_data[train_data['pos'] == pos_tags[2]]['word'].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYZt08ETEEKb",
        "outputId": "6ba1b865-48e5-441b-ae22-91f4ca2f17f9"
      },
      "source": [
        "verb_index = [word_list.index(i) for i in verb]\n",
        "verb_acc = [acc[i] for i in verb_index]\n",
        "verb_f1 = [f1[i] for i in verb_index]\n",
        "noun_index = [word_list.index(i) for i in noun]\n",
        "noun_acc = [acc[i] for i in noun_index]\n",
        "noun_f1 = [f1[i] for i in noun_index]\n",
        "adj_index = [word_list.index(i) for i in adj]\n",
        "adj_acc = [acc[i] for i in adj_index]\n",
        "adj_f1 = [f1[i] for i in adj_index]\n",
        "print (f'Accuracy => verb: {100 * np.mean(verb_acc):0.2f} %  noun: {100*np.mean(noun_acc):0.2f} %  adjective: {100*np.mean(adj_acc):0.2f} %')\n",
        "print (f'F1-Measure => verb: {np.mean(verb_f1):0.2f}  noun: {np.mean(noun_f1):0.2f}   adjective: {np.mean(adj_f1):0.2f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 57.65 %  noun: 60.83 %  adjective: 33.71 %\n",
            "F1-Measure => verb: 0.53  noun: 0.57   adjective: 0.29 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1L8lOb0Zwv"
      },
      "source": [
        "# Bert-LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgUUNwM42NSR"
      },
      "source": [
        "find head index and extract [CLS] [HEAD] [SEP] embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Pkl_UXVwppi"
      },
      "source": [
        "train_data['head_token'] = train_data['head'].apply(lambda x : tokenizer.tokenize(x)[0] )\n",
        "test_data['head_token'] = test_data['head'].apply(lambda x : tokenizer.tokenize(x)[0] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ydnO4fnr45"
      },
      "source": [
        "train_data['cls_head_sep'] = None\n",
        "for i in range(len(train_data)):\n",
        "  head_idx = train_data['tokens'][i].index(train_data['head_token'][i])\n",
        "  train_data['cls_head_sep'][i] = np.array([train_data['embedding'][i][12][0],train_data['embedding'][i][12][head_idx],train_data['embedding'][i][12][-1]])\n",
        "\n",
        "test_data['cls_head_sep'] = None\n",
        "for i in range(len(test_data)):\n",
        "  head_idx = test_data['tokens'][i].index(test_data['head_token'][i])\n",
        "  test_data['cls_head_sep'][i] = np.array([test_data['embedding'][i][12][0],test_data['embedding'][i][12][head_idx],test_data['embedding'][i][12][-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfL5K9SE2rW_"
      },
      "source": [
        "make data ready for input RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ6PtwxRqZVA"
      },
      "source": [
        "data1 = dataset['activate']['cls_head_sep'].values\n",
        "data1 = list(data1)\n",
        "data1 = np.array(data1)\n",
        "\n",
        "data2 = test_dataset['activate']['cls_head_sep'].values\n",
        "data2 = list(data2)\n",
        "data2 = np.array(data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTuT0OnWqenJ"
      },
      "source": [
        "dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  dataset[word]=train_data.loc[train_data['word']==word_list[id]]\n",
        "\n",
        "test_dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  test_dataset[word]=test_data.loc[test_data['word']==word_list[id]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Std21-dnqenK"
      },
      "source": [
        "label_tansformers = {}\n",
        "for word in word_list:\n",
        "  label_tansformers[word] = preprocessing.LabelEncoder().fit(dataset[word]['label'].values)\n",
        "  dataset[word]['label'] = label_tansformers[word].transform(dataset[word]['label'].values)\n",
        "  test_dataset[word]['label'] = test_dataset[word]['label'].map(lambda s: '<unknown>' if s not in label_tansformers[word].classes_ else s)\n",
        "  label_tansformers[word].classes_ = np.append(label_tansformers[word].classes_, '<unknown>')\n",
        "  test_dataset[word]['label'] = label_tansformers[word].transform(test_dataset[word]['label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nOxrgBisGIv"
      },
      "source": [
        "from keras.layers import LSTM ,Dense , Dropout\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du4p7-z83BqM"
      },
      "source": [
        "use keras to build the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtrjTyKIIg6k"
      },
      "source": [
        "def creat_model(classes):\n",
        "  model=Sequential()\n",
        "  model.add(LSTM(30,input_shape=(3,768),activation='relu',return_sequences=True , recurrent_dropout=0.2))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(30,activation='relu' , recurrent_dropout=0.2))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(32,activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(classes,activation='softmax'))\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-S0SBHS29o-",
        "outputId": "1ea87eeb-5837-4de7-ef64-f12c2afa4426"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_23 (LSTM)               (None, 3, 128)            459264    \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 3, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_24 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 595,108\n",
            "Trainable params: 595,108\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAhbAV8V3JEv"
      },
      "source": [
        "train LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zEhN6WHIzNs"
      },
      "source": [
        "acc = []\n",
        "f1 = []\n",
        "for word in word_list:\n",
        "  x = dataset[word]['cls_head_sep'].values\n",
        "  x = list(x)\n",
        "  x = np.array(x)\n",
        "  y = dataset[word]['label'].values\n",
        "  x_t = test_dataset[word]['cls_head_sep'].values\n",
        "  x_t = list(x_t)\n",
        "  x_t = np.array(x_t)\n",
        "  y_t = test_dataset[word]['label'].values\n",
        "  classes = len(label_tansformers[word].classes_)-1\n",
        "  lstm_model = creat_model(classes)\n",
        "  lstm_model.fit(x , y ,epochs=120,batch_size=64)\n",
        "  yp = lstm_model.predict_classes(x_t)\n",
        "  acc.append(accuracy_score(y_t,yp))\n",
        "  f1.append(f1_score(y_t,yp , average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR2nODe8rhjD",
        "outputId": "0a82f9b0-c49c-48a5-8c41-82c466807737"
      },
      "source": [
        "print (f'Accuracy => {100 * np.mean(acc):0.2f} %' )\n",
        "print (f'F1-Measure => {np.mean(f1) :0.2f}' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 70.51 %\n",
            "F1-Measure => 0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BD9o8xdr31B",
        "outputId": "57eb71a6-e941-4b81-b02a-d98c917b10ba"
      },
      "source": [
        "verb_index = [word_list.index(i) for i in verb]\n",
        "verb_acc = [acc[i] for i in verb_index]\n",
        "verb_f1 = [f1[i] for i in verb_index]\n",
        "noun_index = [word_list.index(i) for i in noun]\n",
        "noun_acc = [acc[i] for i in noun_index]\n",
        "noun_f1 = [f1[i] for i in noun_index]\n",
        "adj_index = [word_list.index(i) for i in adj]\n",
        "adj_acc = [acc[i] for i in adj_index]\n",
        "adj_f1 = [f1[i] for i in adj_index]\n",
        "print (f'Accuracy => verb: {100 * np.mean(verb_acc):0.2f} %  noun: {100*np.mean(noun_acc):0.2f} %  adjective: {100*np.mean(adj_acc):0.2f} %')\n",
        "print (f'F1-Measure => verb: {np.mean(verb_f1):0.2f}  noun: {np.mean(noun_f1):0.2f}   adjective: {np.mean(adj_f1):0.2f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 71.99 %  noun: 72.94 %  adjective: 51.91 %\n",
            "F1-Measure => verb: 0.69  noun: 0.72   adjective: 0.49 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}