{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "21YzuzR4hSXL",
        "UDsTnSbslP2M",
        "nySVuuonXBtx",
        "2yy5H2RbXKwU",
        "W6LpnzyZXgil"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9svyXXEjpjl"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZw-mwxRf1ct"
      },
      "source": [
        "!pip install bert-embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbDK2HScgEvr",
        "outputId": "2861bec4-fea3-48b6-dc30-8069e581b37a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from bert_embedding import BertEmbedding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier , VotingClassifier\n",
        "from sklearn.metrics import accuracy_score , f1_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21YzuzR4hSXL"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRl1DjiJwKO"
      },
      "source": [
        "loading data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyPke2fgKNu"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/nlp/HW3/train1.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/nlp/HW3/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8iaagEzJ3_n"
      },
      "source": [
        "remove_tags => a function to omit head tag from the given context\n",
        "\n",
        "find_head => a function to find head word in given the context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtTAWxCjgStr"
      },
      "source": [
        "def remove_tag(contex):\n",
        "  contex = contex.replace(\"<head>\" , \"\")\n",
        "  contex = contex.replace(\"</head>\" , \"\")\n",
        "  return contex\n",
        "\n",
        "import re\n",
        "def find_head(contex):\n",
        "  match = re.findall(r'<head>\\w+</head>' , contex)\n",
        "  return match[0][6:-7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfA3uhOwg6o9"
      },
      "source": [
        "train_data['head'] = train_data['context'].apply(find_head)\n",
        "train_data['context'] = train_data['context'].apply(remove_tag)\n",
        "\n",
        "test_data['head'] = test_data['context'].apply(find_head)\n",
        "test_data['context'] = test_data['context'].apply(remove_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RjXuw1LKbzc"
      },
      "source": [
        "tokenizing context with a simple word tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldvkoSdVg4RF"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "train_data['tokens'] = train_data['context'].apply(tokenizer.tokenize)\n",
        "test_data['tokens'] = test_data['context'].apply(tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Nm7AG1KnIr"
      },
      "source": [
        "seperating a part of the context with the center of head and a specific window size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snI9IpxBgonD"
      },
      "source": [
        "def limiting_contex(data , window_size = 9):\n",
        "  data['limited_context'] = 0\n",
        "  for i in range(len(data)):\n",
        "    idx = data['tokens'][i].index(data['head'][i])\n",
        "    if idx < window_size:\n",
        "      start = 0\n",
        "      stop = idx + (window_size + 1)\n",
        "    elif idx > len(data['tokens']) - (window_size + 1):\n",
        "      start = idx - window_size\n",
        "      stop = len(data['tokens']) - 1\n",
        "    else :\n",
        "      start = idx - window_size\n",
        "      stop = idx + (window_size + 1)\n",
        "    data['limited_context'].iloc[i] = data['tokens'][i][start:stop]\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caTbyuVPjQ8i"
      },
      "source": [
        "train_data = limiting_contex(train_data)\n",
        "test_data = limiting_contex(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcZiUfDTLQxk"
      },
      "source": [
        "detokenize tokens to make a new smaller contetx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr0ShHuYhJLD"
      },
      "source": [
        "train_data['limited_context'] = train_data['limited_context'].apply(TreebankWordDetokenizer().detokenize)\n",
        "test_data['limited_context'] = test_data['limited_context'].apply(TreebankWordDetokenizer().detokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCcOEnMvLc2x"
      },
      "source": [
        "get rid of useless columns in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVidAKjakl_3"
      },
      "source": [
        "train_data = train_data.drop(['instance_id' , 'doc_src' , 'tokens']  , axis=1)\n",
        "train_data = train_data.rename(columns={'sense_id': 'label'})\n",
        "\n",
        "test_data = test_data.drop(['instance_id' , 'doc_src' , 'tokens']  , axis=1)\n",
        "test_data = test_data.rename(columns={'sense_id': 'label'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDsTnSbslP2M"
      },
      "source": [
        "# Bert Embedding and make Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfgcnEi0SkvC"
      },
      "source": [
        "embedding context's tokens with a Bert model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14O3WcT7lTht",
        "outputId": "9fbb4b0c-b92b-4003-9392-c850de95455e"
      },
      "source": [
        "sentences = train_data['limited_context'].values\n",
        "bert_embedding = BertEmbedding()\n",
        "result = bert_embedding(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifOu3pold4n"
      },
      "source": [
        "test_sentences = test_data['limited_context'].values\n",
        "test_result = bert_embedding(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZForjNSyQB"
      },
      "source": [
        "represent with only the the representation of head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8bhSpr7lp2u"
      },
      "source": [
        "def representation1(data , embedding):\n",
        "  rep1 = len(data) * ['0']\n",
        "  for i in range(len(data)):\n",
        "    head_idx = embedding[i][0].index(data['head'][i].lower())\n",
        "    rep1[i] = embedding[i][1][head_idx]\n",
        "  data['rep1'] = rep1\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ5EsOlHmRYj"
      },
      "source": [
        "train_data = representation1(train_data , result)\n",
        "test_data = representation1(test_data , test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxpv1V5LS_3z"
      },
      "source": [
        "calculate tf-idf and make a list that contain tf-idf for each token in each contetx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp6lRAkwmov4"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(sublinear_tf = True , use_idf = True)\n",
        "tfidf = tfidf_vectorizer.fit_transform(train_data['limited_context'])\n",
        "\n",
        "tfidf_doc_word = []\n",
        "f_names = np.array(tfidf_vectorizer.get_feature_names())\n",
        "for i in range(len(train_data)):\n",
        "   _ , index = tfidf[i].nonzero()\n",
        "   my_dict = dict(zip(f_names[index].tolist(), tfidf[i , index].toarray().reshape(-1)))\n",
        "   tfidf_doc_word.append(my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvKQaoMXnZ-R"
      },
      "source": [
        "test_tfidf_vectorizer = TfidfVectorizer(sublinear_tf = True , use_idf = True)\n",
        "test_tfidf = test_tfidf_vectorizer.fit_transform(test_data['limited_context'])\n",
        "\n",
        "test_tfidf_doc_word = []\n",
        "f_names = np.array(test_tfidf_vectorizer.get_feature_names())\n",
        "for i in range(len(test_data)):\n",
        "   _ , index = test_tfidf[i].nonzero()\n",
        "   my_dict = dict(zip(f_names[index].tolist(), test_tfidf[i , index].toarray().reshape(-1)))\n",
        "   test_tfidf_doc_word.append(my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQMeCrdTVQy"
      },
      "source": [
        "represent with a weghted avarage of head neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYA1lzkEnbLR"
      },
      "source": [
        "def representation2(data , embedding , tf_idf):\n",
        "  rep2 = len(data) * ['0']\n",
        "  for i in range(len(data)):\n",
        "    neighbors = []\n",
        "    vec = []\n",
        "    weight = []\n",
        "    tokens = embedding[i][0]\n",
        "    idx = tokens.index(data['head'][i].lower())\n",
        "    for n in range(-3,4):\n",
        "      try:\n",
        "        neighbors.append(tokens[idx + n])\n",
        "      except : continue\n",
        "    for nei in neighbors:\n",
        "      try : \n",
        "        weight.append(tf_idf[i][nei])\n",
        "        vec.append(embedding[i][1][embedding[i][0].index(nei)])\n",
        "      except : continue\n",
        "    rep2[i] = np.average(vec , weights= weight , axis=0)\n",
        "  data['rep2'] = rep2\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NKVOs3MoIbh"
      },
      "source": [
        "train_data = representation2(train_data , result , tfidf_doc_word)\n",
        "test_data = representation2(test_data , test_result , test_tfidf_doc_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7pSFVJQTlKg"
      },
      "source": [
        "represent with a weighted avarage of all tokens in the limited context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5e_X-4PosrA"
      },
      "source": [
        "def representation3(data , embedding , tf_idf):\n",
        "  rep3 = len(data) * ['0']\n",
        "  for i in range(len(data)):\n",
        "    vec = []\n",
        "    weight = []\n",
        "    tokens = embedding[i][0]\n",
        "    for t in tokens:\n",
        "      try : \n",
        "        weight.append(tf_idf[i][t])\n",
        "        vec.append(embedding[i][1][embedding[i][0].index(t)])\n",
        "      except : continue\n",
        "    rep3[i] = np.average(vec , weights= weight , axis=0)\n",
        "  data['rep3'] = rep3\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vziY8HRDpCeC"
      },
      "source": [
        "train_data = representation3(train_data , result , tfidf_doc_word)\n",
        "test_data = representation3(test_data , test_result , test_tfidf_doc_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbD0F4MgT-W8"
      },
      "source": [
        "dimensionality reduction with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooddOmexpXrR"
      },
      "source": [
        "pca = []\n",
        "for i in range(1,4):\n",
        "  pca.append(PCA(n_components=300))\n",
        "  matrix = np.vstack(train_data[f'rep{i}'])\n",
        "  new_rep = pca[i-1].fit_transform(matrix)\n",
        "  new_rep = new_rep.tolist()\n",
        "  new_rep = [np.array(x) for x in new_rep]\n",
        "  train_data[f'rep{i}'] = new_rep\n",
        "\n",
        "for i in range(1,4):\n",
        "  matrix = np.vstack(test_data[f'rep{i}'])\n",
        "  new_rep = pca[i-1].transform(matrix)\n",
        "  new_rep = new_rep.tolist()\n",
        "  new_rep = [np.array(x) for x in new_rep]\n",
        "  test_data[f'rep{i}'] = new_rep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWC-Ox80XUz-"
      },
      "source": [
        "load a pretrained word2vec representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJov8uz3pfi_"
      },
      "source": [
        "import gensim\n",
        "pretrained_embeddings_path = \"/content/drive/MyDrive/nlp/HW3/GoogleNews-vectors-negative300.bin.gz\"\n",
        "word2vec =  gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path,binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x325FrAUXfL4"
      },
      "source": [
        "represent with weighted avarage of neighbors word2vec representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBz9cxXrr_6d"
      },
      "source": [
        "def representation4(data , embedding , tf_idf):\n",
        "  rep4 = len(data) * ['0']\n",
        "  for i in range(len(data)):\n",
        "    neighbors = []\n",
        "    vec = []\n",
        "    weight = []\n",
        "    tokens = tokenizer.tokenize(data['limited_context'][i])\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "    idx = tokens.index(data['head'][i].lower())\n",
        "    for n in range(-3,4):\n",
        "      try:\n",
        "        neighbors.append(tokens[idx + n])\n",
        "      except : continue\n",
        "    for nei in neighbors:\n",
        "      try : \n",
        "        (w , v) = (tf_idf[i][nei] , embedding[nei]) \n",
        "        weight.append(w)\n",
        "        vec.append(v)\n",
        "      except : continue\n",
        "    rep4[i] = np.average(vec , weights= weight , axis=0)\n",
        "  data['rep4'] = rep4\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZTviXBHtSjC"
      },
      "source": [
        "train_data = representation4(train_data , word2vec , tfidf_doc_word)\n",
        "test_data = representation4(test_data , word2vec , test_tfidf_doc_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcrKb8GzXvHo"
      },
      "source": [
        "represent with weighted avarage of all tokens of limited context word2vec representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5yiQnshy3VW"
      },
      "source": [
        "def representation5(data , embedding , tf_idf):\n",
        "  rep5 = len(data) * ['0']\n",
        "  for i in range(len(data)):\n",
        "    vec = []\n",
        "    weight = []\n",
        "    tokens = tokenizer.tokenize(data['limited_context'][i])\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "    for t in tokens:\n",
        "      try : \n",
        "        (w , v) = (tf_idf[i][t] , embedding[t]) \n",
        "        weight.append(w)\n",
        "        vec.append(v)\n",
        "      except : continue\n",
        "    rep5[i] = np.average(vec , weights= weight , axis=0)\n",
        "  data['rep5'] = rep5\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FBcMnINzRnY"
      },
      "source": [
        "train_data = representation5(train_data , word2vec , tfidf_doc_word)\n",
        "test_data = representation5(test_data , word2vec , test_tfidf_doc_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8PS5BYoz9Ym"
      },
      "source": [
        "# Context Classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nySVuuonXBtx"
      },
      "source": [
        "## Part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MtVZk7ufXlW"
      },
      "source": [
        "list all the ambiguous words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr-H8TAL0FiR"
      },
      "source": [
        "word_list = train_data['word'].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyayte7qu9fi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSAQ_TTlfhwH"
      },
      "source": [
        "make dictionary that contains train data for each ambiguous word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63cU_kJm0LY3"
      },
      "source": [
        "dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  dataset[word]=train_data.loc[train_data['word']==word_list[id]]\n",
        "\n",
        "test_dataset = {}\n",
        "for id , word in enumerate(word_list):\n",
        "  test_dataset[word]=test_data.loc[test_data['word']==word_list[id]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um-0y2R4futf"
      },
      "source": [
        "encode sense_ids with appropriate labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCcJHcJ0PHF"
      },
      "source": [
        "label_tansformers = {}\n",
        "for word in word_list:\n",
        "  label_tansformers[word] = preprocessing.LabelEncoder().fit(dataset[word]['label'].values)\n",
        "  dataset[word]['label'] = label_tansformers[word].transform(dataset[word]['label'].values)\n",
        "  test_dataset[word]['label'] = test_dataset[word]['label'].map(lambda s: '<unknown>' if s not in label_tansformers[word].classes_ else s)\n",
        "  label_tansformers[word].classes_ = np.append(label_tansformers[word].classes_, '<unknown>')\n",
        "  test_dataset[word]['label'] = label_tansformers[word].transform(test_dataset[word]['label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7wXfKXfgcOk"
      },
      "source": [
        "a function that get data and classify them based on chosen representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shKXejKMhTlX"
      },
      "source": [
        "def classify (dataset , test_dataset , rep = 'rep1' , classifier = LogisticRegression(random_state=0) , verbose = False):\n",
        "  accuracy_list = []\n",
        "  f1_list = []\n",
        "  model = {}\n",
        "  for word in word_list:\n",
        "    x = np.vstack(dataset[word][rep].values)\n",
        "    y = dataset[word]['label'].values\n",
        "    model[word] = classifier.fit(x,y)\n",
        "    x_t = np.vstack(test_dataset[word][rep].values)\n",
        "    y_t = test_dataset[word]['label'].values\n",
        "    yy = model[word].predict(x_t)\n",
        "    accuracy_list .append(accuracy_score(y_t, yy))\n",
        "    f1_list.append(f1_score(y_t , yy , average='weighted'))\n",
        "  if verbose == True:\n",
        "    print (f'Accuracy => {100 * np.mean(accuracy_list):0.2f} %' )\n",
        "    print (f'F1-Measure => {np.mean(f1_list):0.2f}' )\n",
        "  return accuracy_list , f1_list , model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP8t6k6KmC6Y",
        "outputId": "5fa9b596-f842-4da3-d703-abf138d0d70c"
      },
      "source": [
        "classifire1 = classify(dataset,test_dataset,rep = 'rep1' , classifier = LogisticRegression(random_state=0),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 68.76 %\n",
            "F1-Measure => 0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcSo4Vgzgukc"
      },
      "source": [
        "split words based on their pos tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1ps_EH5pSMo"
      },
      "source": [
        "pos_tags = train_data['pos'].unique().tolist()\n",
        "for word in word_list:\n",
        "  verb = train_data[train_data['pos'] == pos_tags[0]]['word'].unique().tolist()\n",
        "  noun = train_data[train_data['pos'] == pos_tags[1]]['word'].unique().tolist()\n",
        "  adj = train_data[train_data['pos'] == pos_tags[2]]['word'].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz1LBw0Dg37i"
      },
      "source": [
        "classify data and show accuracy and f1-measure for each pos tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV1PXCgPqQkB"
      },
      "source": [
        "def classify_tags(dataset , test_dataset , rep = 'rep1' , classifier = LogisticRegression(random_state=0)):\n",
        "  accuracy , f1 , model= classify(dataset , test_dataset , rep = rep , classifier=classifier)\n",
        "  verb_index = [word_list.index(i) for i in verb]\n",
        "  verb_acc = [accuracy[i] for i in verb_index]\n",
        "  verb_f1 = [f1[i] for i in verb_index]\n",
        "  noun_index = [word_list.index(i) for i in noun]\n",
        "  noun_acc = [accuracy[i] for i in noun_index]\n",
        "  noun_f1 = [f1[i] for i in noun_index]\n",
        "  adj_index = [word_list.index(i) for i in adj]\n",
        "  adj_acc = [accuracy[i] for i in adj_index]\n",
        "  adj_f1 = [f1[i] for i in adj_index]\n",
        "  print (f'Accuracy => verb: {100 * np.mean(verb_acc):0.2f} %  noun: {100*np.mean(noun_acc):0.2f} %  adjective: {100*np.mean(adj_acc):0.2f} %')\n",
        "  print (f'F1-Measure => verb: {np.mean(verb_f1):0.2f}  noun: {np.mean(noun_f1):0.2f}   adjective: {np.mean(adj_f1):0.2f} ')\n",
        "  return {'verb':[verb_acc , verb_f1] , 'noun':[noun_acc, noun_f1] , 'adj':[adj_acc,adj_f1]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiFob_C473IA",
        "outputId": "7f917ae3-6a3b-423e-85c2-4e3777fb8901"
      },
      "source": [
        "acc,f1,model1= classify(dataset,test_dataset,rep = 'rep1' , classifier = LogisticRegression(random_state=10,solver='sag'),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 69.40 %\n",
            "F1-Measure => 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_97T2oNgTTh",
        "outputId": "a4903afa-c718-49c5-f321-3015c06a0d45"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep1', classifier = LogisticRegression(random_state=10,solver='sag'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 70.67 %  noun: 71.46 %  adjective: 53.00 %\n",
            "F1-Measure => verb: 0.67  noun: 0.69   adjective: 0.42 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSr5Ct_Z8QhD",
        "outputId": "53f698f7-5631-4fe4-eb31-d2c312cdd662"
      },
      "source": [
        "acc,f1,model2= classify(dataset,test_dataset,rep = 'rep2' ,classifier = LogisticRegression(random_state=10,solver='sag'),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 63.79 %\n",
            "F1-Measure => 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6fvF7SLlu_B",
        "outputId": "b4c60264-6cb5-4c8e-c76e-ea86aef68d9c"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep2',classifier = LogisticRegression(random_state=10,solver='sag'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 64.09 %  noun: 66.81 %  adjective: 49.86 %\n",
            "F1-Measure => verb: 0.59  noun: 0.64   adjective: 0.43 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJogxYsJ8SvD",
        "outputId": "ce2aaa7b-386d-45e0-9aa6-67408230dc52"
      },
      "source": [
        "acc,f1,model3= classify(dataset,test_dataset,rep = 'rep3' , classifier = LogisticRegression(random_state=10,solver='sag'),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 59.82 %\n",
            "F1-Measure => 0.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1bXsQP-lzOu",
        "outputId": "8e028fa5-d830-4211-9fe8-5edceec3cc82"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep3', classifier = LogisticRegression(random_state=10,solver='sag'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 59.87 %  noun: 62.96 %  adjective: 47.00 %\n",
            "F1-Measure => verb: 0.55  noun: 0.60   adjective: 0.39 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7i92TiQ8SJf",
        "outputId": "93c3c6df-b9b9-4754-bee1-692861c3e36a"
      },
      "source": [
        "acc,f1,model4= classify(dataset,test_dataset,rep = 'rep4' ,classifier = LogisticRegression(random_state=10,solver='sag'),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 54.77 %\n",
            "F1-Measure => 0.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe2KRY4Vl4y9",
        "outputId": "e75260c7-e3d6-4bc0-91f7-0893b057cb97"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep4', classifier = LogisticRegression(random_state=10,solver='sag'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 54.89 %  noun: 58.57 %  adjective: 38.86 %\n",
            "F1-Measure => verb: 0.47  noun: 0.52   adjective: 0.29 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-bEiBR8RkV",
        "outputId": "68e2f468-2530-4975-acc5-248512444069"
      },
      "source": [
        "acc,f1,model5= classify(dataset,test_dataset,rep = 'rep5' , classifier = LogisticRegression(random_state=10,solver='sag'),verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 53.50 %\n",
            "F1-Measure => 0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_65oV8l73O",
        "outputId": "504ce008-036f-4da6-9324-94f1da2c82d0"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep5', classifier = LogisticRegression(random_state=10,solver='sag'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 54.43 %  noun: 55.67 %  adjective: 38.86 %\n",
            "F1-Measure => verb: 0.47  noun: 0.48   adjective: 0.28 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ3krvKF8RBc",
        "outputId": "eecce740-bc6b-4b17-9fc0-cdbfabd9bf4b"
      },
      "source": [
        "acc,f1,model6 = classify(dataset,test_dataset,rep = 'rep1' , classifier = RandomForestClassifier(max_depth=4, random_state=0) ,verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 64.26 %\n",
            "F1-Measure => 0.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSH5Al1xmAUc",
        "outputId": "276887df-1353-47a4-e2df-710e729f0b24"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep1', classifier = RandomForestClassifier(max_depth=4, random_state=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 65.32 %  noun: 66.94 %  adjective: 46.71 %\n",
            "F1-Measure => verb: 0.58  noun: 0.62   adjective: 0.38 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr1494mv8n-M",
        "outputId": "980f6cfe-f782-4bd1-939d-939403e9f6c2"
      },
      "source": [
        "acc,f1,model7 = classify(dataset,test_dataset,rep = 'rep2' , classifier = RandomForestClassifier(max_depth=4, random_state=0) ,verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 54.08 %\n",
            "F1-Measure => 0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJCfzSPPmMwB",
        "outputId": "169b6667-4ca9-4480-9252-22151f2d41e7"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep2', classifier = RandomForestClassifier(max_depth=4, random_state=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 53.62 %  noun: 58.63 %  adjective: 38.86 %\n",
            "F1-Measure => verb: 0.45  noun: 0.51   adjective: 0.28 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj0xRosI8ola",
        "outputId": "04bdab85-557d-4645-b7f1-7168a1d16a16"
      },
      "source": [
        "acc,f1,model8 = classify(dataset,test_dataset,rep = 'rep3' , classifier = RandomForestClassifier(max_depth=4, random_state=0) ,verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 51.15 %\n",
            "F1-Measure => 0.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhJOxjMkmOun",
        "outputId": "b766f3d7-4956-4c63-d359-ab0af8220946"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep3', classifier = RandomForestClassifier(max_depth=4, random_state=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 52.03 %  noun: 52.81 %  adjective: 38.86 %\n",
            "F1-Measure => verb: 0.43  noun: 0.44   adjective: 0.28 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1t6fskM8pA9",
        "outputId": "95a7d647-929b-414c-fd8b-5b3284f0dac6"
      },
      "source": [
        "acc,f1,model9 = classify(dataset,test_dataset,rep = 'rep4' , classifier = RandomForestClassifier(max_depth=4, random_state=0) ,verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 51.67 %\n",
            "F1-Measure => 0.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Xm0cTUmSYV",
        "outputId": "71afc2fa-9091-40ca-d000-59d325d3cf0a"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep4', classifier = RandomForestClassifier(max_depth=4, random_state=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 51.77 %  noun: 54.71 %  adjective: 38.86 %\n",
            "F1-Measure => verb: 0.44  noun: 0.46   adjective: 0.29 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEccx8jx8pb3",
        "outputId": "5ddd8e3f-a6af-4372-d60f-0a5e850d8a93"
      },
      "source": [
        "acc,f1,model10 = classify(dataset,test_dataset,rep = 'rep5' , classifier = RandomForestClassifier(max_depth=4, random_state=0) ,verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 52.32 %\n",
            "F1-Measure => 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIMKBM7emVUZ",
        "outputId": "38713a77-8c90-4763-8f80-58c629459868"
      },
      "source": [
        "dic = classify_tags(dataset,test_dataset,rep = 'rep5', classifier = RandomForestClassifier(max_depth=4, random_state=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 52.92 %  noun: 53.78 %  adjective: 42.71 %\n",
            "F1-Measure => verb: 0.45  noun: 0.45   adjective: 0.32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yy5H2RbXKwU"
      },
      "source": [
        "## Part B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jyABeBcYj_r"
      },
      "source": [
        "a function to find most frequent itme in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irtU4GGwFYn6"
      },
      "source": [
        "def most_frequent(List):\n",
        "    return max(set(List), key = List.count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bKZzhvAYyEA"
      },
      "source": [
        "train 3 best model on their own data and vote between them to find the ensemble label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrk8J1IeLbcK",
        "outputId": "4eb48a57-47e4-4484-de64-4c79a456338f"
      },
      "source": [
        "accuracy= []\n",
        "f1= []\n",
        "for word in word_list:\n",
        "  x = np.vstack(dataset[word]['rep1'].values)\n",
        "  y = dataset[word]['label'].values\n",
        "  model1 = LogisticRegression(random_state=10,solver='sag').fit(x,y)\n",
        "  x_t = np.vstack(test_dataset[word]['rep1'].values)\n",
        "  y_t = test_dataset[word]['label'].values\n",
        "  yy1 = model1.predict(x_t)\n",
        "  x = np.vstack(dataset[word]['rep2'].values)\n",
        "  model2 = LogisticRegression(random_state=10,solver='sag').fit(x,y)\n",
        "  x_t = np.vstack(test_dataset[word]['rep2'].values)\n",
        "  yy2 = model2.predict(x_t)\n",
        "  x = np.vstack(dataset[word]['rep1'].values)\n",
        "  model3 = RandomForestClassifier(max_depth=4, random_state=0).fit(x,y)\n",
        "  x_t = np.vstack(test_dataset[word]['rep1'].values)\n",
        "  yy3 = model3.predict(x_t)\n",
        "  yy = yy1\n",
        "  for i in range(len(yy1)):\n",
        "    a = [yy1[i] , yy2[i] , yy3[i]]\n",
        "    yy[i] = most_frequent(a)\n",
        "  accuracy .append(accuracy_score(y_t, yy))\n",
        "  f1.append(f1_score(y_t , yy , average='weighted'))\n",
        "print (f'Accuracy => {100 * np.mean(accuracy):0.2f} %' )\n",
        "print (f'F1-Measure => {np.mean(f1):0.2f}' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 68.34 %\n",
            "F1-Measure => 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWJn5U4UZBEk"
      },
      "source": [
        "calculate the accuracy and f1-measure for each pos tag on ensemble model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKTDZ3tyQyM3",
        "outputId": "d896ac4c-c362-474d-8e88-6665cb2adb79"
      },
      "source": [
        "verb_index = [word_list.index(i) for i in verb]\n",
        "verb_acc = [accuracy[i] for i in verb_index]\n",
        "verb_f1 = [f1[i] for i in verb_index]\n",
        "noun_index = [word_list.index(i) for i in noun]\n",
        "noun_acc = [accuracy[i] for i in noun_index]\n",
        "noun_f1 = [f1[i] for i in noun_index]\n",
        "adj_index = [word_list.index(i) for i in adj]\n",
        "adj_acc = [accuracy[i] for i in adj_index]\n",
        "adj_f1 = [f1[i] for i in adj_index]\n",
        "print (f'Accuracy => verb: {100 * np.mean(verb_acc):0.2f} %  noun: {100*np.mean(noun_acc):0.2f} %  adjective: {100*np.mean(adj_acc):0.2f} %')\n",
        "print (f'F1-Measure => verb: {np.mean(verb_f1):0.2f}  noun: {np.mean(noun_f1):0.2f}   adjective: {np.mean(adj_f1):0.2f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 69.95 %  noun: 70.08 %  adjective: 51.00 %\n",
            "F1-Measure => verb: 0.65  noun: 0.66   adjective: 0.39 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6LpnzyZXgil"
      },
      "source": [
        "## Part C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuns7jib4Gkg"
      },
      "source": [
        "a function that return predicted labels for dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HPGZFnBMPJ3"
      },
      "source": [
        "def predict(classifier , rep = 'rep1'):\n",
        "  pred = {}\n",
        "  for word in word_list:\n",
        "    x = np.vstack(dataset[word][rep].values)\n",
        "    y = dataset[word]['label'].values\n",
        "    model = classifier.fit(x,y)\n",
        "    x_t = np.vstack(test_dataset[word][rep].values)\n",
        "    y_t = test_dataset[word]['label'].values\n",
        "    yy = model.predict(x_t)\n",
        "    pred[word] = yy\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8qR3LzJ4Vsm"
      },
      "source": [
        "get lables from each three model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUMUytTYMype"
      },
      "source": [
        "pred1 = predict(LogisticRegression(random_state=0) , rep = 'rep1')\n",
        "pred2 = predict(LogisticRegression(random_state=0) , rep = 'rep2')\n",
        "pred3 = predict(RandomForestClassifier(max_depth=4, random_state=0) , rep = 'rep1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2Jy2LS4bgY"
      },
      "source": [
        "make the input vector from labels for the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8qnA9ldNCwl"
      },
      "source": [
        "pred = {}\n",
        "for word in word_list:\n",
        "  pred[word]= np.array([pred1[word],pred2[word],pred3[word]])\n",
        "  pred[word] = pred[word].transpose()\n",
        "\n",
        "y_true = {}\n",
        "for word in word_list:\n",
        "  y_true[word] = test_dataset[word]['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1HQObAv4mv6"
      },
      "source": [
        "train a decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLLINQU3Qz0P",
        "outputId": "654ff154-3a5e-4f66-da71-f36405c18986"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "accuracy , f1 = [] , []\n",
        "for word in word_list:\n",
        "  DT = DecisionTreeClassifier(random_state=0, max_depth=2).fit(pred[word],y_true[word])\n",
        "  accuracy.append(DT.score(pred[word],y_true[word]))\n",
        "  f1.append(DT.score(pred[word],y_true[word]))\n",
        "\n",
        "print (f'Accuracy => {100 * np.mean(accuracy):0.2f} %' )\n",
        "print (f'F1-Measure => {np.mean(f1):0.2f}' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => 77.34 %\n",
            "F1-Measure => 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4ol53ySZDT",
        "outputId": "5862fe58-c004-4335-ce2e-808f3686422c"
      },
      "source": [
        "verb_index = [word_list.index(i) for i in verb]\n",
        "verb_acc = [accuracy[i] for i in verb_index]\n",
        "verb_f1 = [f1[i] for i in verb_index]\n",
        "noun_index = [word_list.index(i) for i in noun]\n",
        "noun_acc = [accuracy[i] for i in noun_index]\n",
        "noun_f1 = [f1[i] for i in noun_index]\n",
        "adj_index = [word_list.index(i) for i in adj]\n",
        "adj_acc = [accuracy[i] for i in adj_index]\n",
        "adj_f1 = [f1[i] for i in adj_index]\n",
        "print (f'Accuracy => verb: {100 * np.mean(verb_acc):0.2f} %  noun: {100*np.mean(noun_acc):0.2f} %  adjective: {100*np.mean(adj_acc):0.2f} %')\n",
        "print (f'F1-Measure => verb: {np.mean(verb_f1):0.2f}  noun: {np.mean(noun_f1):0.2f}   adjective: {np.mean(adj_f1):0.2f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy => verb: 77.48 %  noun: 79.28 %  adjective: 68.71 %\n",
            "F1-Measure => verb: 0.77  noun: 0.79   adjective: 0.69 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}